{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "#import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import partial\n",
    "#import mwparserfromhell\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import guidedlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"/scratch/nh1724/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model_eta.pkl              \u001b[0m\u001b[38;5;27mInference-topic-model\u001b[0m/\r\n",
      "base_model_guidedlda_all.pkl    results_lda_top.pth\r\n",
      "base_model_guidedlda.pkl        results.pth\r\n",
      "base_model_guidedlda_top.pkl    run-jupyter.sbatch\r\n",
      "base_model.pkl                  slurm-6162026.out\r\n",
      "en_outlinks_tokens_df.pkl       \u001b[38;5;27msquad\u001b[0m/\r\n",
      "en_tokens_lem_stem_df.pkl       \u001b[48;5;10;38;5;21msquad-QA-char\u001b[0m/\r\n",
      "features.pkl                    \u001b[38;5;27mtest\u001b[0m/\r\n",
      "features_top.pkl                text_embed_en.pkl\r\n",
      "graph_df.pkl                    \u001b[38;5;34mwikitext_tokenized_text_sections_outlinks_en.p\u001b[0m*\r\n",
      "inference_project_teresa.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls $PATH_TO_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pkl.load(open(PATH_TO_DATA + \"wikitext_tokenized_text_sections_outlinks_en.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nh1724/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(57)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(list_of_tokens):\n",
    "    output = []\n",
    "    for token in list_of_tokens:\n",
    "        output.append(stemmer.stem(lemmatizer.lemmatize(token, pos='v')))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'do', 'do', 'procrastin', 'wolv', 'cat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "lemmatize_stemming(\"I did doing procrastination wolves cats\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del wiki_df[\"raw_tokens\"]\n",
    "wiki_df.tokens = wiki_df.tokens.apply(lemmatize_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sections_tokens</th>\n",
       "      <th>raw_outlinks</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tokens_lem_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6199</td>\n",
       "      <td>[History_And_Society.History and society, Hist...</td>\n",
       "      <td>[anarch, anti, authoritarian, anti, authoritar...</td>\n",
       "      <td>[etymology, terminology, definition, history, ...</td>\n",
       "      <td>[[[Anti-authoritarianism|anti-authoritarian]],...</td>\n",
       "      <td>[Anti-authoritarianism, political philosophy, ...</td>\n",
       "      <td>[anarch, anti, authoritarian, anti, authoritar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q38404</td>\n",
       "      <td>[STEM.Medicine, STEM.Biology, History_And_Soci...</td>\n",
       "      <td>[autism, development, disord, character, diffi...</td>\n",
       "      <td>[characteristics, social, development, communi...</td>\n",
       "      <td>[[[Psychiatry]], [[Interpersonal relationship|...</td>\n",
       "      <td>[Psychiatry, Interpersonal relationship, commu...</td>\n",
       "      <td>[autism, development, disord, character, diffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q101038</td>\n",
       "      <td>[STEM.Physics, STEM.Space, History_And_Society...</td>\n",
       "      <td>[sunlight, relat, various, surfac, condit, alb...</td>\n",
       "      <td>[terrestrial, albedo, white, sky, black, sky, ...</td>\n",
       "      <td>[[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "      <td>[File:Albedo-e hg.svg, diffuse reflection, sun...</td>\n",
       "      <td>[sunlight, relat, various, surfac, condit, alb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q173</td>\n",
       "      <td>[Geography.Americas]</td>\n",
       "      <td>[alabama, alabama, nicknam, northern, flicker,...</td>\n",
       "      <td>[etymology, history, pre, european, settlement...</td>\n",
       "      <td>[[[Coat of arms of Alabama|Coat of arms]], [[N...</td>\n",
       "      <td>[Coat of arms of Alabama, Northern flicker, Di...</td>\n",
       "      <td>[alabama, alabama, nicknam, northern, flicker,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q41746</td>\n",
       "      <td>[Culture.People, Geography.Europe, History_And...</td>\n",
       "      <td>[date, three, zero, zero, bc, achill, kill, et...</td>\n",
       "      <td>[etymology, birth, early, years, names, hidden...</td>\n",
       "      <td>[[[File:Achilles fighting against Memnon Leide...</td>\n",
       "      <td>[File:Achilles fighting against Memnon Leiden ...</td>\n",
       "      <td>[date, three, zero, zero, bc, achill, kill, et...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       QID                               mid_level_categories  \\\n",
       "0    Q6199  [History_And_Society.History and society, Hist...   \n",
       "1   Q38404  [STEM.Medicine, STEM.Biology, History_And_Soci...   \n",
       "2  Q101038  [STEM.Physics, STEM.Space, History_And_Society...   \n",
       "3     Q173                               [Geography.Americas]   \n",
       "4   Q41746  [Culture.People, Geography.Europe, History_And...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [anarch, anti, authoritarian, anti, authoritar...   \n",
       "1  [autism, development, disord, character, diffi...   \n",
       "2  [sunlight, relat, various, surfac, condit, alb...   \n",
       "3  [alabama, alabama, nicknam, northern, flicker,...   \n",
       "4  [date, three, zero, zero, bc, achill, kill, et...   \n",
       "\n",
       "                                     sections_tokens  \\\n",
       "0  [etymology, terminology, definition, history, ...   \n",
       "1  [characteristics, social, development, communi...   \n",
       "2  [terrestrial, albedo, white, sky, black, sky, ...   \n",
       "3  [etymology, history, pre, european, settlement...   \n",
       "4  [etymology, birth, early, years, names, hidden...   \n",
       "\n",
       "                                        raw_outlinks  \\\n",
       "0  [[[Anti-authoritarianism|anti-authoritarian]],...   \n",
       "1  [[[Psychiatry]], [[Interpersonal relationship|...   \n",
       "2  [[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...   \n",
       "3  [[[Coat of arms of Alabama|Coat of arms]], [[N...   \n",
       "4  [[[File:Achilles fighting against Memnon Leide...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [Anti-authoritarianism, political philosophy, ...   \n",
       "1  [Psychiatry, Interpersonal relationship, commu...   \n",
       "2  [File:Albedo-e hg.svg, diffuse reflection, sun...   \n",
       "3  [Coat of arms of Alabama, Northern flicker, Di...   \n",
       "4  [File:Achilles fighting against Memnon Leiden ...   \n",
       "\n",
       "                                     tokens_lem_stem  \n",
       "0  [anarch, anti, authoritarian, anti, authoritar...  \n",
       "1  [autism, development, disord, character, diffi...  \n",
       "2  [sunlight, relat, various, surfac, condit, alb...  \n",
       "3  [alabama, alabama, nicknam, northern, flicker,...  \n",
       "4  [date, three, zero, zero, bc, achill, kill, et...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df[\"tokens_lem_stem\"] = wiki_df[\"tokens\"]\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 45, \n",
      "\n",
      "['Culture.Architecture' 'Culture.Arts' 'Culture.Broadcasting'\n",
      " 'Culture.Crafts and hobbies' 'Culture.Entertainment'\n",
      " 'Culture.Food and drink' 'Culture.Games and toys'\n",
      " 'Culture.Internet culture' 'Culture.Language and literature'\n",
      " 'Culture.Media' 'Culture.Music' 'Culture.People'\n",
      " 'Culture.Performing arts' 'Culture.Philosophy and religion'\n",
      " 'Culture.Sports' 'Culture.Visual arts' 'Geography.Africa'\n",
      " 'Geography.Americas' 'Geography.Antarctica' 'Geography.Asia'\n",
      " 'Geography.Bodies of water' 'Geography.Europe' 'Geography.Landforms'\n",
      " 'Geography.Maps' 'Geography.Oceania' 'Geography.Parks'\n",
      " 'History_And_Society.Business and economics'\n",
      " 'History_And_Society.Education' 'History_And_Society.History and society'\n",
      " 'History_And_Society.Military and warfare'\n",
      " 'History_And_Society.Politics and government'\n",
      " 'History_And_Society.Transportation' 'STEM.Biology' 'STEM.Chemistry'\n",
      " 'STEM.Engineering' 'STEM.Geosciences' 'STEM.Information science'\n",
      " 'STEM.Mathematics' 'STEM.Medicine' 'STEM.Meteorology' 'STEM.Physics'\n",
      " 'STEM.Science' 'STEM.Space' 'STEM.Technology' 'STEM.Time']\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(wiki_df.mid_level_categories)\n",
    "print(f\"Number of categories: {len(mlb.classes_)}, \\n\\n{mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(wiki_df, y, test_size=0.1, random_state=42) #all train, output features\n",
    "#X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to output features, use full set for X, y\n",
    "X_train = wiki_df\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute prior eta based on a subset of whole corpus\n",
    "def init_eta(y, bow_corpus, dictionary, num_examples=1000):\n",
    "    '''\n",
    "    y: MultiLabelBinarizer array, (num_doc, num_topic)\n",
    "    bow_corpus: bag of word corpus\n",
    "    dictionary: word vocab\n",
    "    num_examples: restrict on a smaller subset as initialization\n",
    "    '''\n",
    "    eta = np.zeros(y.T.shape)\n",
    "    y = y.astype(int) #ensure integer type\n",
    "    for i in range(num_examples):\n",
    "        key_idx = np.array(list(dict(bow_corpus[i]).keys()))\n",
    "        counts = np.array(list(dict(bow_corpus[i]).values()))\n",
    "        key_idx = key_idx.astype(int) #ensure integer type\n",
    "        eta[np.ix_(y[i]==1,key_idx)] += counts\n",
    "    return eta/np.sum(eta) #normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(X_train.tokens)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5,keep_n=y_train.shape[0]) #keep same shape as num of examples to initialize the prior (can improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in X_train.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33823, 33823)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus), len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33823, 45)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta0 = init_eta(y_train, bow_corpus,dictionary, num_examples=3000) #use small subset to guess seeded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 33823)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## guidedLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create seeded topic for GuidedLDA (list of list)\n",
    "topic_words = []\n",
    "n_top_words=10\n",
    "for i in range(45): #45 topics\n",
    "    topic_words_i = [dictionary[idx] for idx in np.array(dictionary)[np.argsort(eta0[i])][:-(n_top_words+1):-1]]\n",
    "    topic_words.append(topic_words_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Culture.Architecture</th>\n",
       "      <th>Culture.Arts</th>\n",
       "      <th>Culture.Broadcasting</th>\n",
       "      <th>Culture.Crafts and hobbies</th>\n",
       "      <th>Culture.Entertainment</th>\n",
       "      <th>Culture.Food and drink</th>\n",
       "      <th>Culture.Games and toys</th>\n",
       "      <th>Culture.Internet culture</th>\n",
       "      <th>Culture.Language and literature</th>\n",
       "      <th>Culture.Media</th>\n",
       "      <th>...</th>\n",
       "      <th>STEM.Geosciences</th>\n",
       "      <th>STEM.Information science</th>\n",
       "      <th>STEM.Mathematics</th>\n",
       "      <th>STEM.Medicine</th>\n",
       "      <th>STEM.Meteorology</th>\n",
       "      <th>STEM.Physics</th>\n",
       "      <th>STEM.Science</th>\n",
       "      <th>STEM.Space</th>\n",
       "      <th>STEM.Technology</th>\n",
       "      <th>STEM.Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>build</td>\n",
       "      <td>art</td>\n",
       "      <td>big</td>\n",
       "      <td>card</td>\n",
       "      <td>film</td>\n",
       "      <td>food</td>\n",
       "      <td>game</td>\n",
       "      <td>game</td>\n",
       "      <td>languag</td>\n",
       "      <td>music</td>\n",
       "      <td>...</td>\n",
       "      <td>island</td>\n",
       "      <td>librari</td>\n",
       "      <td>mean</td>\n",
       "      <td>cell</td>\n",
       "      <td>ice</td>\n",
       "      <td>energi</td>\n",
       "      <td>system</td>\n",
       "      <td>star</td>\n",
       "      <td>system</td>\n",
       "      <td>ndash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>design</td>\n",
       "      <td>museum</td>\n",
       "      <td>brother</td>\n",
       "      <td>bank</td>\n",
       "      <td>anim</td>\n",
       "      <td>product</td>\n",
       "      <td>player</td>\n",
       "      <td>internet</td>\n",
       "      <td>write</td>\n",
       "      <td>media</td>\n",
       "      <td>...</td>\n",
       "      <td>earth</td>\n",
       "      <td>leibniz</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>human</td>\n",
       "      <td>forc</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>earth</td>\n",
       "      <td>comput</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>door</td>\n",
       "      <td>collect</td>\n",
       "      <td>seri</td>\n",
       "      <td>debit</td>\n",
       "      <td>seri</td>\n",
       "      <td>milk</td>\n",
       "      <td>chess</td>\n",
       "      <td>search</td>\n",
       "      <td>word</td>\n",
       "      <td>game</td>\n",
       "      <td>...</td>\n",
       "      <td>miner</td>\n",
       "      <td>univers</td>\n",
       "      <td>distribut</td>\n",
       "      <td>effect</td>\n",
       "      <td>atmospher</td>\n",
       "      <td>electron</td>\n",
       "      <td>centuri</td>\n",
       "      <td>space</td>\n",
       "      <td>engin</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>centuri</td>\n",
       "      <td>bc</td>\n",
       "      <td>season</td>\n",
       "      <td>knot</td>\n",
       "      <td>dub</td>\n",
       "      <td>plant</td>\n",
       "      <td>franklin</td>\n",
       "      <td>user</td>\n",
       "      <td>book</td>\n",
       "      <td>film</td>\n",
       "      <td>...</td>\n",
       "      <td>water</td>\n",
       "      <td>copyright</td>\n",
       "      <td>frac</td>\n",
       "      <td>diseas</td>\n",
       "      <td>temperatur</td>\n",
       "      <td>physic</td>\n",
       "      <td>theori</td>\n",
       "      <td>orbit</td>\n",
       "      <td>oper</td>\n",
       "      <td>calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art</td>\n",
       "      <td>centuri</td>\n",
       "      <td>show</td>\n",
       "      <td>reserv</td>\n",
       "      <td>charact</td>\n",
       "      <td>produc</td>\n",
       "      <td>play</td>\n",
       "      <td>network</td>\n",
       "      <td>english</td>\n",
       "      <td>blue</td>\n",
       "      <td>...</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>carnegi</td>\n",
       "      <td>data</td>\n",
       "      <td>studi</td>\n",
       "      <td>earth</td>\n",
       "      <td>forc</td>\n",
       "      <td>comput</td>\n",
       "      <td>system</td>\n",
       "      <td>design</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>mtv</td>\n",
       "      <td>feder</td>\n",
       "      <td>batman</td>\n",
       "      <td>process</td>\n",
       "      <td>hand</td>\n",
       "      <td>googl</td>\n",
       "      <td>publish</td>\n",
       "      <td>jazz</td>\n",
       "      <td>...</td>\n",
       "      <td>rock</td>\n",
       "      <td>columbia</td>\n",
       "      <td>normal</td>\n",
       "      <td>medic</td>\n",
       "      <td>effect</td>\n",
       "      <td>particl</td>\n",
       "      <td>color</td>\n",
       "      <td>light</td>\n",
       "      <td>data</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>architectur</td>\n",
       "      <td>file</td>\n",
       "      <td>music</td>\n",
       "      <td>currenc</td>\n",
       "      <td>comic</td>\n",
       "      <td>water</td>\n",
       "      <td>nintendo</td>\n",
       "      <td>irc</td>\n",
       "      <td>letter</td>\n",
       "      <td>advertis</td>\n",
       "      <td>...</td>\n",
       "      <td>carbon</td>\n",
       "      <td>book</td>\n",
       "      <td>statist</td>\n",
       "      <td>caus</td>\n",
       "      <td>corioli</td>\n",
       "      <td>system</td>\n",
       "      <td>scienc</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>program</td>\n",
       "      <td>politician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wall</td>\n",
       "      <td>british</td>\n",
       "      <td>doctor</td>\n",
       "      <td>nickel</td>\n",
       "      <td>show</td>\n",
       "      <td>fruit</td>\n",
       "      <td>kasparov</td>\n",
       "      <td>video</td>\n",
       "      <td>centuri</td>\n",
       "      <td>news</td>\n",
       "      <td>...</td>\n",
       "      <td>ice</td>\n",
       "      <td>public</td>\n",
       "      <td>comput</td>\n",
       "      <td>system</td>\n",
       "      <td>water</td>\n",
       "      <td>atom</td>\n",
       "      <td>univers</td>\n",
       "      <td>moon</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>museum</td>\n",
       "      <td>room</td>\n",
       "      <td>film</td>\n",
       "      <td>attle</td>\n",
       "      <td>releas</td>\n",
       "      <td>beer</td>\n",
       "      <td>video</td>\n",
       "      <td>meme</td>\n",
       "      <td>univers</td>\n",
       "      <td>communic</td>\n",
       "      <td>...</td>\n",
       "      <td>metal</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>right</td>\n",
       "      <td>blood</td>\n",
       "      <td>rotat</td>\n",
       "      <td>mass</td>\n",
       "      <td>languag</td>\n",
       "      <td>observ</td>\n",
       "      <td>languag</td>\n",
       "      <td>footbal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>file</td>\n",
       "      <td>ad</td>\n",
       "      <td>televis</td>\n",
       "      <td>system</td>\n",
       "      <td>award</td>\n",
       "      <td>chocol</td>\n",
       "      <td>tile</td>\n",
       "      <td>troll</td>\n",
       "      <td>film</td>\n",
       "      <td>mickey</td>\n",
       "      <td>...</td>\n",
       "      <td>iron</td>\n",
       "      <td>digit</td>\n",
       "      <td>peirc</td>\n",
       "      <td>organ</td>\n",
       "      <td>climat</td>\n",
       "      <td>univers</td>\n",
       "      <td>human</td>\n",
       "      <td>planet</td>\n",
       "      <td>air</td>\n",
       "      <td>author</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Culture.Architecture Culture.Arts Culture.Broadcasting  \\\n",
       "0                build          art                  big   \n",
       "1               design       museum              brother   \n",
       "2                 door      collect                 seri   \n",
       "3              centuri           bc               season   \n",
       "4                  art      centuri                 show   \n",
       "5                  jpg          jpg                  mtv   \n",
       "6          architectur         file                music   \n",
       "7                 wall      british               doctor   \n",
       "8               museum         room                 film   \n",
       "9                 file           ad              televis   \n",
       "\n",
       "  Culture.Crafts and hobbies Culture.Entertainment Culture.Food and drink  \\\n",
       "0                       card                  film                   food   \n",
       "1                       bank                  anim                product   \n",
       "2                      debit                  seri                   milk   \n",
       "3                       knot                   dub                  plant   \n",
       "4                     reserv               charact                 produc   \n",
       "5                      feder                batman                process   \n",
       "6                    currenc                 comic                  water   \n",
       "7                     nickel                  show                  fruit   \n",
       "8                      attle                releas                   beer   \n",
       "9                     system                 award                 chocol   \n",
       "\n",
       "  Culture.Games and toys Culture.Internet culture  \\\n",
       "0                   game                     game   \n",
       "1                 player                 internet   \n",
       "2                  chess                   search   \n",
       "3               franklin                     user   \n",
       "4                   play                  network   \n",
       "5                   hand                    googl   \n",
       "6               nintendo                      irc   \n",
       "7               kasparov                    video   \n",
       "8                  video                     meme   \n",
       "9                   tile                    troll   \n",
       "\n",
       "  Culture.Language and literature Culture.Media  ... STEM.Geosciences  \\\n",
       "0                         languag         music  ...           island   \n",
       "1                           write         media  ...            earth   \n",
       "2                            word          game  ...            miner   \n",
       "3                            book          film  ...            water   \n",
       "4                         english          blue  ...             nbsp   \n",
       "5                         publish          jazz  ...             rock   \n",
       "6                          letter      advertis  ...           carbon   \n",
       "7                         centuri          news  ...              ice   \n",
       "8                         univers      communic  ...            metal   \n",
       "9                            film        mickey  ...             iron   \n",
       "\n",
       "  STEM.Information science STEM.Mathematics STEM.Medicine STEM.Meteorology  \\\n",
       "0                  librari             mean          cell              ice   \n",
       "1                  leibniz        algorithm         human             forc   \n",
       "2                  univers        distribut        effect        atmospher   \n",
       "3                copyright             frac        diseas       temperatur   \n",
       "4                  carnegi             data         studi            earth   \n",
       "5                 columbia           normal         medic           effect   \n",
       "6                     book          statist          caus          corioli   \n",
       "7                   public           comput        system            water   \n",
       "8               dictionari            right         blood            rotat   \n",
       "9                    digit            peirc         organ           climat   \n",
       "\n",
       "  STEM.Physics STEM.Science STEM.Space STEM.Technology   STEM.Time  \n",
       "0       energi       system       star          system       ndash  \n",
       "1         nbsp         nbsp      earth          comput         day  \n",
       "2     electron      centuri      space           engin    american  \n",
       "3       physic       theori      orbit            oper    calendar  \n",
       "4         forc       comput     system          design       month  \n",
       "5      particl        color      light            data     english  \n",
       "6       system       scienc       nbsp         program  politician  \n",
       "7         atom      univers       moon            nbsp      player  \n",
       "8         mass      languag     observ         languag     footbal  \n",
       "9      univers        human     planet             air      author  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show topic and top topic_words\n",
    "df_topic = pd.DataFrame(topic_words)\n",
    "df_topic = df_topic.T\n",
    "df_topic.columns = mlb.classes_\n",
    "df_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics = {}\n",
    "for t_id, st in enumerate(topic_words):\n",
    "    for word in st:\n",
    "        seed_topics[dictionary.token2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GuidedLDA with seed words init\n",
    "#model = guidedlda.GuidedLDA(n_topics=45, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = {idx:0 for idx in range(16)}\n",
    "topic_map.update({idx:1 for idx in range(16,26,1)})\n",
    "topic_map.update({idx:2 for idx in range(26,32,1)})\n",
    "topic_map.update({idx:3 for idx in range(32,45,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GuidedLDA with only 4 broad topics\n",
    "seed_topics_top = {}\n",
    "for idx, st in enumerate(topic_words):\n",
    "    for word in st:\n",
    "        top_id = topic_map[idx]\n",
    "        seed_topics_top[dictionary.token2id[word]] = top_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse input matrix from corpus\n",
    "X = gensim.matutils.corpus2csc(bow_corpus,dtype=int).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33823, 33823)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = guidedlda.GuidedLDA(n_topics=45, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)\n",
    "#model.fit(X, seed_topics=seed_topics_top, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pkl.dump(model, open(PATH_TO_DATA + \"base_model_guidedlda_all.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model: all the words, to create visualization\n",
    "model = pkl.load(open(PATH_TO_DATA + \"base_model_guidedlda_all.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 33823)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topic_word_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['england', 'london', 'british', 'king', 'kingdom', 'royal', 'john', 'william', 'hous', 'english']\n",
      "1 ['templ', 'centuri', 'build', 'jpg', 'king', 'architectur', 'file', 'stone', 'palac', 'ancient']\n",
      "2 ['women', 'social', 'children', 'sexual', 'person', 'sex', 'group', 'child', 'age', 'femal']\n",
      "3 ['bank', 'tax', 'currenc', 'financi', 'money', 'coin', 'account', 'rate', 'exchang', 'issu']\n",
      "4 ['film', 'award', 'seri', 'charact', 'best', 'releas', 'star', 'role', 'actor', 'comic']\n",
      "5 ['food', 'plant', 'product', 'fruit', 'produc', 'oil', 'grow', 'crop', 'seed', 'milk']\n",
      "6 ['hand', 'game', 'wear', 'ball', 'often', 'chess', 'move', 'fire', 'usual', 'player']\n",
      "7 ['user', 'window', 'network', 'servic', 'softwar', 'web', 'internet', 'version', 'releas', 'googl']\n",
      "8 ['languag', 'word', 'english', 'write', 'speak', 'dialect', 'linguist', 'script', 'vowel', 'letter']\n",
      "9 ['day', 'televis', 'media', 'report', 'news', 'broadcast', 'show', 'tv', 'channel', 'radio']\n",
      "10 ['album', 'song', 'music', 'record', 'band', 'releas', 'award', 'tour', 'perform', 'singl']\n",
      "11 ['write', 'book', 'life', 'publish', 'death', 'novel', 'father', 'stori', 'die', 'would']\n",
      "12 ['music', 'play', 'cultur', 'art', 'danc', 'tradit', 'centuri', 'perform', 'literatur', 'popular']\n",
      "13 ['god', 'tradit', 'text', 'christian', 'religion', 'church', 'buddhism', 'accord', 'buddhist', 'religi']\n",
      "14 ['cup', 'open', 'final', 'team', 'match', 'footbal', 'round', 'lose', 'tournament', 'tenni']\n",
      "15 ['file', 'jpg', 'flag', 'color', 'art', 'paint', 'red', 'white', 'design', 'imag']\n",
      "16 ['al', 'islam', 'arab', 'empir', 'muslim', 'roman', 'centuri', 'bc', 'greek', 'egypt']\n",
      "17 ['game', 'season', 'team', 'olymp', 'leagu', 'sport', 'summer', 'championship', 'play', 'player']\n",
      "18 ['india', 'indian', 'sri', 'tamil', 'british', 'bengal', 'lanka', 'gandhi', 'hindu', 'south']\n",
      "19 ['district', 'india', 'pakistan', 'khan', 'indian', 'singh', 'delhi', 'nbsp', 'bangladesh', 'nepal']\n",
      "20 ['river', 'area', 'region', 'provinc', 'district', 'lake', 'north', 'mountain', 'popul', 'south']\n",
      "21 ['german', 'russian', 'germani', 'soviet', 'russia', 'war', 'europ', 'european', 'union', 'republ']\n",
      "22 ['china', 'chines', 'dynasti', 'japan', 'japanes', 'tibetan', 'han', 'asia', 'mongol', 'tibet']\n",
      "23 ['build', 'line', 'nbsp', 'construct', 'bridg', 'measur', 'design', 'map', 'wall', 'structur']\n",
      "24 ['south', 'africa', 'australia', 'african', 'zealand', 'countri', 'australian', 'korea', 'rugbi', 'north']\n",
      "25 ['de', 'french', 'franc', 'la', 'spanish', 'spain', 'san', 'pari', 'italian', 'saint']\n",
      "26 ['compani', 'econom', 'product', 'market', 'industri', 'busi', 'countri', 'manag', 'trade', 'increas']\n",
      "27 ['univers', 'school', 'educ', 'student', 'scienc', 'institut', 'colleg', 'research', 'award', 'studi']\n",
      "28 ['citi', 'area', 'popul', 'build', 'station', 'town', 'district', 'school', 'locat', 'railway']\n",
      "29 ['war', 'armi', 'forc', 'militari', 'attack', 'battl', 'command', 'kill', 'british', 'fight']\n",
      "30 ['govern', 'parti', 'elect', 'presid', 'polit', 'minist', 'law', 'member', 'right', 'constitut']\n",
      "31 ['air', 'aircraft', 'airport', 'flight', 'oper', 'airlin', 'launch', 'intern', 'forc', 'servic']\n",
      "32 ['speci', 'anim', 'plant', 'bird', 'human', 'famili', 'group', 'live', 'breed', 'biolog']\n",
      "33 ['acid', 'chemic', 'metal', 'nbsp', 'reaction', 'oxid', 'carbon', 'element', 'compound', 'product']\n",
      "34 ['power', 'electr', 'engin', 'current', 'nbsp', 'voltag', 'system', 'signal', 'circuit', 'frequenc']\n",
      "35 ['island', 'philippin', 'sea', 'indonesia', 'dutch', 'east', 'provinc', 'south', 'languag', 'coast']\n",
      "36 ['american', 'counti', 'york', 'indiana', 'california', 'washington', 'america', 'white', 'south', 'north']\n",
      "37 ['frac', 'text', 'right', 'leav', 'align', 'valu', 'mathbf', 'mathemat', 'function', 'style']\n",
      "38 ['cell', 'diseas', 'caus', 'blood', 'medic', 'treatment', 'infect', 'effect', 'protein', 'patient']\n",
      "39 ['water', 'gas', 'heat', 'air', 'temperatur', 'surfac', 'ice', 'pressur', 'caus', 'ocean']\n",
      "40 ['energi', 'physic', 'field', 'light', 'particl', 'mass', 'forc', 'electron', 'wave', 'magnet']\n",
      "41 ['theori', 'social', 'studi', 'human', 'philosophi', 'natur', 'univers', 'scienc', 'societi', 'idea']\n",
      "42 ['star', 'earth', 'orbit', 'moon', 'planet', 'solar', 'sun', 'space', 'nbsp', 'system']\n",
      "43 ['system', 'comput', 'data', 'program', 'process', 'languag', 'inform', 'design', 'code', 'exampl']\n",
      "44 ['ndash', 'american', 'english', 'politician', 'player', 'footbal', 'actor', 'singer', 'author', 'french']\n"
     ]
    }
   ],
   "source": [
    "#word topic assignment\n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = [dictionary[t] for t in np.argsort(topic_dist)[:-(n_top_words+1):-1]]\n",
    "    print(str(i), topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "/home/nh1724/.conda/envs/lda/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n"
     ]
    }
   ],
   "source": [
    "#document topics assignment\n",
    "doc_topic = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(doc_topic, open(PATH_TO_DATA + \"features.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_metrics_dict(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset.\n",
    "    \"\"\"\n",
    "    # macro precision, recall, f-score\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"macro\"\n",
    "    )\n",
    "    # micro precision, recall, f-score\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"micro\"\n",
    "    )\n",
    "    # combine all metrics in a dict\n",
    "    dict_metrics = {\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro, \n",
    "        \"recall_micro\": recall_micro, \n",
    "        \"f1_micro\": f1_micro,\n",
    "    }\n",
    "    # round\n",
    "    n_digits = 3\n",
    "    dict_metrics = {\n",
    "        metric_name: round(value, n_digits) \n",
    "        for metric_name, value in dict_metrics.items()\n",
    "    }\n",
    "    return dict_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold: each doc topic sum to 1, threshold topic w/ prob > 1/45\n",
    "from copy import deepcopy\n",
    "y_pred = deepcopy(doc_topic)\n",
    "y_pred = np.where(y_pred>1/45, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_dict(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pkl.load(open(PATH_TO_DATA + \"features.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#due to the huge dataset and a small vocab, all zero row in document-term matrix exists. This is to ensure we look at the valid doc cases\n",
    "valid_idx = np.where(row_sum==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45, 33823), (2749, 45))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word.shape, doc_topic[valid_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(dictionary.values()) #list of terms in the dictionary\n",
    "vocab_tf = [dict(i) for i in bow_corpus]\n",
    "vocab_tf = list(pd.DataFrame(vocab_tf).sum(axis=0)) #list of term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate document lenghts based on bow corpus\n",
    "doc_lengths = np.array([len(article) for article in bow_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results for visz:https://github.com/vi3k6i5/GuidedLDA/issues/23\n",
    "visz = {'topic_term_dists':topic_word,\n",
    "        'doc_topic_dists':doc_topic[valid_idx],\n",
    "        'doc_lengths': doc_lengths[valid_idx],\n",
    "        'vocab':vocab, \n",
    "        'term_frequency':vocab_tf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(visz,open(PATH_TO_DATA + \"visz_all.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nh1724/.conda/envs/lda/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "#import visz data\n",
    "data = pkl.load(open(PATH_TO_DATA + \"visz_all.pkl\", \"rb\"))\n",
    "import pyLDAvis\n",
    "# prepare the data\n",
    "tef_vis_data = pyLDAvis.prepare(**data)\n",
    "\n",
    "# this bit needs to be run after running the earlier code for reasons\n",
    "pyLDAvis.display(tef_vis_data)\n",
    "\n",
    "# save to HTML\n",
    "pyLDAvis.save_html(tef_vis_data, \"LDAvis_all.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda",
   "language": "python",
   "name": "lda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
