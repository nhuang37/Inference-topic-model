{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "from dgl.data import BitcoinOTC\n",
    "import datetime\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import importlib.util\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def module_from_file(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "graph_utils = module_from_file(\"graph_utils\", \"../graph_utils.py\")\n",
    "gen_utils = module_from_file(\"gen_utils\", \"../utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = 300\n",
    "hid_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0\n",
    "learning_rate = 0.0001\n",
    "wt_decay = 0\n",
    "stpsize = 20\n",
    "n_epochs = 30\n",
    "n_classes = 45\n",
    "out_path = '/misc/vlgscratch4/BrunaGroup/rj1408/inference/models/iter1/'\n",
    "self_loop = True\n",
    "data_path = '/misc/vlgscratch4/BrunaGroup/rj1408/inference/data/'\n",
    "graph_file = 'graph_df.pkl'\n",
    "feature_file = 'text_embed_en.pkl'\n",
    "label_file = 'en_outlinks_tokens_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSelfEdges(edgeList, colFrom, colTo):\n",
    "    mask = edgeList[:, colFrom] - edgeList[:, colTo] != 0\n",
    "    edgeList = edgeList[mask]\n",
    "    return edgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(data, self_loop):\n",
    "    g = graph_utils.build_graph(data)\n",
    "    if self_loop == True:\n",
    "        g.add_edges(g.nodes(), g.nodes())\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "with open(os.path.join(data_path, graph_file), \"rb\") as f:\n",
    "    wiki_graph_df = pkl.load(f)\n",
    "    \n",
    "graph = load_graphs(wiki_graph_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load feature dataframes\n",
    "with open(os.path.join(data_path, feature_file), \"rb\") as f:\n",
    "    wiki_feature_df = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(data_path, label_file), \"rb\") as f:\n",
    "    wiki_label_df = pkl.load(f)\n",
    "    \n",
    "joined_df = wiki_feature_df.join(wiki_graph_df, lsuffix='1')\n",
    "joined_df = joined_df.join(wiki_label_df, lsuffix='2').sort_values(by='node_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = joined_df['text_1000_embed']\n",
    "features = torch.stack(features.tolist())\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(joined_df.mid_level_categories)\n",
    "labels = torch.FloatTensor(labels)\n",
    "graph.ndata['feat'] = features\n",
    "graph.ndata['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample training/validation and test\n",
    "train_p = 0.6\n",
    "val_p = 0.15\n",
    "test_p = 0.25\n",
    "\n",
    "indices = np.arange(graph.number_of_nodes())\n",
    "indices_train,indices_test = train_test_split(indices, test_size=test_p, random_state=42)\n",
    "indices_train, indices_val = train_test_split(indices_train, train_size = train_p/(1 - test_p), random_state=42)\n",
    "train_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "val_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "test_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "train_mask[indices_train] = True\n",
    "val_mask[indices_val] = True\n",
    "test_mask[indices_test] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For class imbalance\n",
    "pos_examples = np.sum(labels[train_mask].long().numpy(), axis = 0)\n",
    "neg_examples = labels[train_mask].shape[0] - pos_examples\n",
    "weights = neg_examples/pos_examples\n",
    "weights = torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_layers,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        \n",
    "        # output layer\n",
    "        self.outlayer = nn.Linear(n_hidden, n_classes, bias = True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g):\n",
    "        h = features\n",
    "        print(\"Got input: \")\n",
    "        print(h)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "            print(\"i: \", i)\n",
    "            print(h)\n",
    "        outputs = self.outlayer(h)\n",
    "        print(\"output\")\n",
    "        print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, criterion, device, graph, mask):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        feat = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feat, graph)\n",
    "        labels = graph.ndata['labels']\n",
    "        labels = labels.to(device)\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "        loss = criterion(outputs, labels)\n",
    "            \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, criterion, device, graph, mask):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        feat = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feat, graph)\n",
    "        outputs = outputs[mask]\n",
    "                    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, criterion, device, graph, mask, labels):\n",
    "    logits = predict(model, criterion, device, graph, mask)\n",
    "    preds = (logits > 0).long()\n",
    "    ground_labels = labels[mask].long()\n",
    "    dict_metrics = gen_utils.get_metrics_dict(ground_labels, preds)\n",
    "    return dict_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for supervised training\n",
    "def train_model(model, criterion, optimizer, scheduler, device, checkpoint_path, hyperparams, graph, train_mask, val_mask, num_epochs=25):\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"train\"] = {}\n",
    "    metrics_dict[\"valid\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"][\"epochwise\"] = []\n",
    "    metrics_dict[\"valid\"][\"loss\"] = {}\n",
    "    metrics_dict[\"valid\"][\"loss\"][\"epochwise\"] = []\n",
    "        \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 9999999999999999\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{} \\n'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        print('\\n')\n",
    "        \n",
    "        #train phase\n",
    "        scheduler.step()\n",
    "        model.train() \n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        forward_start_time  = time.time()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            feats = graph.ndata['feat'].to(device)\n",
    "            outputs = model(feats, graph)\n",
    "            labels = graph.ndata['labels']\n",
    "            labels = labels.to(device)\n",
    "            outputs = outputs[train_mask]\n",
    "            labels = labels[train_mask]\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        forward_time = time.time() - forward_start_time\n",
    "        \n",
    "        print('Train Loss: {:.4f} \\n'.format(epoch_loss))\n",
    "        metrics_dict[\"train\"][\"loss\"][\"epochwise\"].append(epoch_loss)\n",
    "        \n",
    "        #validation phase\n",
    "        val_epoch_loss = evaluate_loss(model, criterion, device, graph, val_mask)\n",
    "        print('Validation Loss: {:.4f} \\n'.format(val_epoch_loss))\n",
    "        metrics_dict[\"valid\"][\"loss\"][\"epochwise\"].append(val_epoch_loss)\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'full_metrics': metrics_dict,\n",
    "        'hyperparams': hyperparams\n",
    "        }, '%s/net_epoch_%d.pth' % (checkpoint_path, epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s \\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f} \\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create GCN model\n",
    "model = GCN(node_dim, hid_dim, n_layers, n_classes, F.relu, dropout)\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=learning_rate, weight_decay = wt_decay)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=stpsize, gamma=0.1)\n",
    "hyper_params = {'node_dim' : node_dim,\n",
    "    'hid_dim': hid_dim,\n",
    "    'n_layers' : n_layers,\n",
    "    'dropout' : dropout,\n",
    "    'wt_decay' : wt_decay,\n",
    "    }\n",
    "\n",
    "bst_model = train_model(model, criterion, optimizer, exp_lr_scheduler, device, out_path, hyper_params, graph, train_mask, val_mask, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
