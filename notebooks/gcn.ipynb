{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "from dgl.data import BitcoinOTC\n",
    "import datetime\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import importlib.util\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def module_from_file(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "graph_utils = module_from_file(\"graph_utils\", \"../graph_utils.py\")\n",
    "gen_utils = module_from_file(\"gen_utils\", \"../utils.py\")\n",
    "topic_utils = module_from_file(\"topic_utils\", \"../utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = 300\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0\n",
    "learning_rate = 0.01\n",
    "wt_decay = 0\n",
    "stpsize = 5000\n",
    "n_epochs = 1000\n",
    "n_classes = 45\n",
    "out_path = '/misc/vlgscratch4/BrunaGroup/rj1408/inference/models/iter1/'\n",
    "self_loop = True\n",
    "data_path = '/misc/vlgscratch4/BrunaGroup/rj1408/inference/data/'\n",
    "graph_file = 'graph_df.pkl'\n",
    "feature_file = 'text_embed_en.pkl'\n",
    "label_file = 'en_outlinks_tokens_df.pkl'\n",
    "activation = F.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSelfEdges(edgeList, colFrom, colTo):\n",
    "    mask = edgeList[:, colFrom] - edgeList[:, colTo] != 0\n",
    "    edgeList = edgeList[mask]\n",
    "    return edgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(data, self_loop):\n",
    "    g = graph_utils.build_graph(data)\n",
    "    if self_loop == True:\n",
    "        g.add_edges(g.nodes(), g.nodes())\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph\n",
    "with open(os.path.join(data_path, graph_file), \"rb\") as f:\n",
    "    wiki_graph_df = pkl.load(f)\n",
    "    \n",
    "graph = load_graphs(wiki_graph_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load feature dataframes\n",
    "with open(os.path.join(data_path, feature_file), \"rb\") as f:\n",
    "    wiki_feature_df = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(data_path, label_file), \"rb\") as f:\n",
    "    wiki_label_df = pkl.load(f)\n",
    "    \n",
    "joined_df = wiki_feature_df.join(wiki_graph_df, lsuffix='1')\n",
    "joined_df = joined_df.join(wiki_label_df, lsuffix='2').sort_values(by='node_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = joined_df['text_1000_embed']\n",
    "features = np.nan_to_num(np.stack(features))\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(joined_df.mid_level_categories)\n",
    "labels = torch.FloatTensor(labels)\n",
    "graph.ndata['feat'] = torch.FloatTensor(features)\n",
    "graph.ndata['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample training/validation and test\n",
    "train_p = 0.6\n",
    "val_p = 0.15\n",
    "test_p = 0.25\n",
    "\n",
    "indices = np.arange(graph.number_of_nodes())\n",
    "indices_train,indices_test = train_test_split(indices, test_size=test_p, random_state=42)\n",
    "indices_train, indices_val = train_test_split(indices_train, train_size = train_p/(1 - test_p), random_state=42)\n",
    "train_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "val_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "test_mask = torch.zeros(graph.number_of_nodes()).bool()\n",
    "train_mask[indices_train] = True\n",
    "val_mask[indices_val] = True\n",
    "test_mask[indices_test] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For class imbalance\n",
    "pos_examples = np.sum(labels[train_mask].long().numpy(), axis = 0)\n",
    "neg_examples = labels[train_mask].shape[0] - pos_examples\n",
    "psweights = neg_examples/pos_examples\n",
    "psweights = torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For interclass imbalance\n",
    "max_examples = np.max(pos_examples)\n",
    "class_weights = max_examples/pos_examples\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_layers,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        \n",
    "        # output layer\n",
    "        self.outlayer = nn.Linear(n_hidden, n_classes, bias = True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        outputs = self.outlayer(h)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, criterion, device, graph, mask):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        feat = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feat, graph)\n",
    "        labels = graph.ndata['labels']\n",
    "        labels = labels.to(device)\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = loss*class_weights\n",
    "        loss = torch.mean(loss)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, criterion, device, graph, mask):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        feat = graph.ndata['feat'].to(device)\n",
    "        outputs = model(feat, graph)\n",
    "        outputs = outputs[mask]\n",
    "                    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, criterion, device, graph, mask, labels):\n",
    "    logits = predict(model, criterion, device, graph, mask)\n",
    "    preds = (logits > 0).long()\n",
    "    ground_labels = labels[mask].long()\n",
    "    dict_metrics = gen_utils.get_metrics_dict(ground_labels, preds)\n",
    "    return dict_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for supervised training\n",
    "def train_model(model, criterion, optimizer, scheduler, device, checkpoint_path, hyperparams, graph, train_mask, val_mask, num_epochs=25, check_iter=50):\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"train\"] = {}\n",
    "    metrics_dict[\"valid\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"][\"epochwise\"] = []\n",
    "    metrics_dict[\"valid\"][\"loss\"] = {}\n",
    "    metrics_dict[\"valid\"][\"loss\"][\"epochwise\"] = []\n",
    "        \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 9999999999999999\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #train phase\n",
    "        scheduler.step()\n",
    "        model.train() \n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        forward_start_time  = time.time()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            feats = graph.ndata['feat'].to(device)\n",
    "            outputs = model(feats, graph)\n",
    "            labels = graph.ndata['labels']\n",
    "            labels = labels.to(device)\n",
    "            outputs = outputs[train_mask]\n",
    "            labels = labels[train_mask]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss*class_weights\n",
    "            loss = torch.mean(loss)\n",
    "            epoch_loss = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        forward_time = time.time() - forward_start_time\n",
    "        \n",
    "        \n",
    "        metrics_dict[\"train\"][\"loss\"][\"epochwise\"].append(epoch_loss)\n",
    "        \n",
    "        #validation phase\n",
    "        val_epoch_loss = evaluate_loss(model, criterion, device, graph, val_mask)\n",
    "        metrics_dict[\"valid\"][\"loss\"][\"epochwise\"].append(val_epoch_loss)\n",
    "        \n",
    "        if epoch%check_iter==0:\n",
    "            print('Epoch {}/{} \\n'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('\\n')\n",
    "            print('Train Loss: {:.4f} \\n'.format(epoch_loss))\n",
    "            print('Validation Loss: {:.4f} \\n'.format(val_epoch_loss))\n",
    "        \n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss < best_loss:\n",
    "            best_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if epoch%check_iter==0:\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'full_metrics': metrics_dict,\n",
    "            'hyperparams': hyperparams\n",
    "            }, '%s/net_epoch_%d.pth' % (checkpoint_path, epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s \\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f} \\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 25.1099 \n",
      "\n",
      "Validation Loss: 20.9552 \n",
      "\n",
      "Epoch 50/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 5.3910 \n",
      "\n",
      "Validation Loss: 5.4193 \n",
      "\n",
      "Epoch 100/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 4.0666 \n",
      "\n",
      "Validation Loss: 4.1054 \n",
      "\n",
      "Epoch 150/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 3.4064 \n",
      "\n",
      "Validation Loss: 3.4436 \n",
      "\n",
      "Epoch 200/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 3.0915 \n",
      "\n",
      "Validation Loss: 3.1529 \n",
      "\n",
      "Epoch 250/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.9588 \n",
      "\n",
      "Validation Loss: 3.0158 \n",
      "\n",
      "Epoch 300/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.8257 \n",
      "\n",
      "Validation Loss: 2.9144 \n",
      "\n",
      "Epoch 350/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.7866 \n",
      "\n",
      "Validation Loss: 2.8570 \n",
      "\n",
      "Epoch 400/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.7098 \n",
      "\n",
      "Validation Loss: 2.8235 \n",
      "\n",
      "Epoch 450/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.6635 \n",
      "\n",
      "Validation Loss: 2.7473 \n",
      "\n",
      "Epoch 500/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.6020 \n",
      "\n",
      "Validation Loss: 2.7078 \n",
      "\n",
      "Epoch 550/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.5580 \n",
      "\n",
      "Validation Loss: 2.6924 \n",
      "\n",
      "Epoch 600/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.5922 \n",
      "\n",
      "Validation Loss: 2.6604 \n",
      "\n",
      "Epoch 650/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.4855 \n",
      "\n",
      "Validation Loss: 2.6173 \n",
      "\n",
      "Epoch 700/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.4914 \n",
      "\n",
      "Validation Loss: 2.6063 \n",
      "\n",
      "Epoch 750/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.4770 \n",
      "\n",
      "Validation Loss: 2.6352 \n",
      "\n",
      "Epoch 800/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.3959 \n",
      "\n",
      "Validation Loss: 2.5370 \n",
      "\n",
      "Epoch 850/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.3581 \n",
      "\n",
      "Validation Loss: 2.5124 \n",
      "\n",
      "Epoch 900/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.3454 \n",
      "\n",
      "Validation Loss: 2.5193 \n",
      "\n",
      "Epoch 950/999 \n",
      "\n",
      "----------\n",
      "\n",
      "\n",
      "Train Loss: 2.3758 \n",
      "\n",
      "Validation Loss: 2.5769 \n",
      "\n",
      "Training complete in 1m 37s \n",
      "\n",
      "Best val loss: 2.456958 \n",
      "\n"
     ]
    }
   ],
   "source": [
    " # create GCN model\n",
    "model = GCN(node_dim, hid_dim, n_layers, n_classes, activation, dropout)\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()#pos_weight=class_weights.to(device), reduction='none')\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=learning_rate, weight_decay = wt_decay)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=stpsize, gamma=0.1)\n",
    "hyper_params = {'node_dim' : node_dim,\n",
    "    'hid_dim': hid_dim,\n",
    "    'n_layers' : n_layers,\n",
    "    'dropout' : dropout,\n",
    "    'wt_decay' : wt_decay,\n",
    "    }\n",
    "\n",
    "bst_model = train_model(model, criterion, optimizer, exp_lr_scheduler, device, out_path, hyper_params, graph, train_mask, val_mask, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logits(model, device, graph, mask=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = graph.ndata['feat'].to(device)\n",
    "        logits = model(features, graph)\n",
    "        \n",
    "        if mask is not None:\n",
    "            logits = logits[mask]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_logits = predict_logits(bst_model, device, graph, test_mask)\n",
    "hard_predictions = (predicted_logits > 0).long()\n",
    "hard_predictions = hard_predictions.cpu().detach().numpy()\n",
    "ground_truth_labels = labels[test_mask].long().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.712,\n",
       " 'recall_macro': 0.391,\n",
       " 'f1_macro': 0.473,\n",
       " 'precision_micro': 0.826,\n",
       " 'recall_micro': 0.596,\n",
       " 'f1_micro': 0.693}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_utils.get_metrics_dict(ground_truth_labels, hard_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
